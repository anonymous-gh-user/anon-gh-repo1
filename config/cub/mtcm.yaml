seed: 0
debug: false

mode: cbm-base
exp_name: cbm-base
checkpoint_dir: /home/ANON-USER/scratch/ckpt/${data.dataset}/${mode}_fold=${data.fold}_j${slurm_job_id}
save_weights: false

pretrained: false
pretraining:
  data: breast
  style: mae

from_ckpt: /h/ANON-USER/medcbr/_checkpoint/${cbm.backbone}-${pretraining.data}-${pretraining.style}/fold${data.fold}.pth

wandb:
  name: ${exp_name}_fold=${data.fold}_slurmid=${slurm_job_id}
  project: MedCBM
  group: cbm_base
  tags: 
  log_images: true

slurm:
  qos: m3
  mem: 48G
  gpu: gpu:rtx6000:1
  time: 8:00:00
  cpus: 8

data:
  dataset: CUB
  data_dir: /h/ANON-USER/medcbr/data/${data.dataset}
  fold: 0
  num_folds: 5
  splitting: holdout
  sampling: undersample
  sampling_ratio: 1.0
  crop_rois: false # whether to crop the images to the ROIs
  image_size: 384
  batch_size: 64
  augmentations:
    - translation
    - rotate
    - flip

  subset_train_data: 1.0
  use_llm_output: false
  datasets: ["BrEaST", "BUSBRA"] # datasets to use, can be ["BrEaST", "BUSBRA", "ALL"]

training:
  num_epochs: 300
  lr_scheduler: cosine
  pretrained: true
  from_ckpt: null
  accumulate_grad_steps: 1

optimizer:
  name: sgd
  encoder_lr: 1e-02
  main_lr: 1e-02
  wd: 0 # weight decay
  encoder_frozen_epochs: 0
  encoder_warmup_epochs: 5
  main_frozen_epochs: 0
  main_warmup_epochs: 5

cbm:
  num_concepts: 112
  num_birads: 6 # 0, 1, 2, 3, 4, 5
  num_classes: 200
  concepts: [1, 4, 6, 7, 10, 14, 15, 20, 21, 23, 25, 29, 30, 35, 36, 38,
    40, 44, 45, 50, 51, 53, 54, 56, 57, 59, 63, 64, 69, 70, 72,
    75, 80, 84, 90, 91, 93, 99, 101, 106, 110, 111, 116, 117, 119,
    125, 126, 131, 132, 134, 145, 149, 151, 152, 153, 157, 158, 163,
    164, 168, 172, 178, 179, 181, 183, 187, 188, 193, 194, 196, 198,
    202, 203, 208, 209, 211, 212, 213, 218, 220, 221, 225, 235, 236,
    238, 239, 240, 242, 243, 244, 249, 253, 254, 259, 260, 262, 268,
    274, 277, 283, 289, 292, 293, 294, 298, 299, 304, 305, 308, 309,
    310, 311]
  backbone: resnet50
  architecture: cmh
  gamma: 1.0
  concept_weights: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]

metrics:
  tune_threshold: true

learning_rate: 0.001
loss: ce_and_concept_mse_loss

device: cuda
use_amp: false

#python3 run.py -o wandb.group=new_losses_rn50_joint data.fold=1 loss=concept_weighted_ce_loss
#python3 run.py -y cbm_base -o wandb.group=LaBo_CBM_smolGamma cbm.gamma=0.3 loss=unweighted_multi_task_ce_loss data.fold=0