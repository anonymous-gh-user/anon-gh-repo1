seed: 0
debug: false

mode: reasoning
exp_name: ${backbone}-bus
checkpoint_dir: /home/ANON-USER/scratch/ckpt/${data.dataset}/${mode}_fold=${data.fold}_j${slurm_job_id}
save_weights: false

pretrained: false
pretraining:
  data: breast
  style: supervised

from_ckpt: /h/ANON-USER/medcbr/_checkpoint/${backbone}-${pretraining.data}-${pretraining.style}/fold${data.fold}.pth

wandb:
  name: ${exp_name}_fold=${data.fold}_slurmid=${slurm_job_id}
  project: MedCBM
  group: reasoning
  tags: 
  log_images: true

slurm:
  qos: long
  mem: 48G
  gpu: gpu:a40:1
  time: 48:00:00

data:
  dataset: CUB
  data_dir: /h/ANON-USER/medcbr/data/${data.dataset}
  fold: 0
  num_folds: 5
  splitting: holdout
  sampling: divide
  sampling_ratio: 1.0
  crop_rois: false
  use_llm_output: false
  image_size: 224

  batch_size: 1
  augmentations:
    - translation

  datasets: ["BrEaST", "BUSBRA"] # datasets to use, can be ["BrEaST", "BUSBRA", "ALL"]

training:
  num_epochs: 1
  lr_scheduler: cosine
  pretrained: true
  from_ckpt: null
  accumulate_grad_steps: 1

optimizer:
  name: adamw
  encoder_lr: 1e-05
  main_lr: 1e-05
  wd: 1e-4 # weight decay
  encoder_frozen_epochs: 0
  encoder_warmup_epochs: 0
  main_frozen_epochs: 0
  main_warmup_epochs: 0

backbone: qwen
num_classes: 2

llama:
  prompt_mode: guidelines
  include_mask: false
  include_bbox: false

concept_model: 
  cfg: /home/ANON-USER/projects/aip-anonlab/ANON-USER/medcbr/config/cub/ours_clip_vit_l.yaml
  ckpt: /home/ANON-USER/projects/aip-anonlab/ANON-USER/medcbr/.model-weights/${data.dataset}/medcbr/seed${seed}.pth

cbm:
  num_concepts: 112
  num_birads: 6 # 0, 1, 2, 3, 4, 5
  num_classes: 200
  concepts: [1, 4, 6, 7, 10, 14, 15, 20, 21, 23, 25, 29, 30, 35, 36, 38,
    40, 44, 45, 50, 51, 53, 54, 56, 57, 59, 63, 64, 69, 70, 72,
    75, 80, 84, 90, 91, 93, 99, 101, 106, 110, 111, 116, 117, 119,
    125, 126, 131, 132, 134, 145, 149, 151, 152, 153, 157, 158, 163,
    164, 168, 172, 178, 179, 181, 183, 187, 188, 193, 194, 196, 198,
    202, 203, 208, 209, 211, 212, 213, 218, 220, 221, 225, 235, 236,
    238, 239, 240, 242, 243, 244, 249, 253, 254, 259, 260, 262, 268,
    274, 277, 283, 289, 292, 293, 294, 298, 299, 304, 305, 308, 309,
    310, 311]
  architecture: multihead # cbl, multihead, fusion
  concept_weights: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]

learning_rate: 0.001
loss: cross_entropy

device: cuda
use_amp: false

# python3 run.py -y llama -o wandb.group=LLaMA_LaBo_1 data.fold=0