# ðŸ©» MedCBR: Vision-Language Models Encode Clinical Guidelines for Concept-Based Reasoning

###  Dataset setup
##### BUS-BRA
1. Download the data
2. Move the images into the correct directory
3. Preprocess the data by cropping to lesion ROIs: 
```python3
data/run_preprocessing.py -d busbra -src <src_directory> -dst <target_directory>
```
In our work, we did 
```python3
data/run_preprocessing.py -d busbra -src data/BUSBRA -dst data/BUSBRA_cropped
```

##### BrEaST
Follow the same steps as BUSBRA, e.g.
```python3
data/run_preprocessing.py -d busbra -src data/BrEaST -dst data/BrEaST_cropped
```

##### CBIS-DDSM & CUB-200-2011
1. Download the data
2. Move the images into the correct directory

--- 

### Model Training
Our code base supports the following models. Any model not mentioned here was trained using code provided by its creators. 

#### Supported Models
**Black-box Vision Models**:
- ResNet50
- CLIP RN50 
- CLIP ViT-B/32
- CLIP ViT-L/14
- SigLIP
- BiomedCLIP

**Concept-based models**:
- Original CBM (Koh et al., 2020)
- MTCM
- CLIP CBM

To run a model using k-fold cross-validation, we used:
```bash
scripts/run_job.sh -y <yaml_name> -g <wandb_group_name> -f <fold> -t <time>
```
To run a model using different random seeds, we used:
```bash
scripts/run_job.sh -y <yaml_name> -g <wandb_group_name> -s <seed> -t <time>
```

#### Unsupported Models
The following models must be cloned from their respective repositories and run according to the instructions provided by their authors:
- PCBM, PCBMh (Yuksekgonul et al., ICLR 2023)
- Label-free CBM (Oikarinen et al., ICLR 2023)
- AdaCBM (Chowdhury et al., MICCAI 2024)

---

### Synthetic Report Generation

To generate a synthetic report, we run the following code:
```bash
scripts/run_job.sh -y <dataset>/qwen2vl -g <wandb_group_name> -f <fold> -t <time>
```
This will run the Qwen2.5VL LVLM on the test set of that fold. The 3 YAMLs `busbra/qwen2vl`, `ddsm/qwen2vl`, and `cub/qwen2vl` specify the hyperparameters to use for this run. 

The `src/run_qwenvl.py` file contains the logic used to run the LVLM on the data to generate reports. The model and prompt are both implemented in `src/models/qwen.py`, and the guidelines are recorded in `src/utils/clinical_guideline.py` (although some of the guidelines are non-clinical).

### CLIP Concept Training
To train a multi-task concept model using CLIP, we use the following code:
```bash
scripts/run_job.sh -y <dataset>/medcbr -g <wandb_group_name> -f <fold> -t <time>
```
This will train a train a CLIP model with ViT-L/14 backbone on the data. The hyperparameters are found in YAMLs `busbra/medcbr`, `ddsm/medcbr`, and `cub/medcbr`. In particular, the hyperparameter
```yaml
clip:
    ...
    use_llm_output: true
    ...
```
is responsible for enabling CLIP-based training on the guideline-conformant reports generated by the LVLM. The weights for the multi-task loss are also recorded as:
```yaml
  clip_weight: 1.0
  det_weight: 5.0
  concept_weight: 1.0
```

To train a CLIP CBM baseline, use the following script:
```bash
scripts/run_job.sh -y <dataset>/clip_cbm -g <wandb_group_name> -f <fold> -t <time>
```
which has the following values in the YAML:
```yaml
  clip_weight: 0.0
  det_weight: 1.0
  concept_weight: 1.0
  use_llm_output: false
```

### Clinical Reasoning
Finally, to generate the reasoning using Qwen3, run the following:
```bash
scripts/run_job.sh -y <dataset>/qwen3 -g <wandb_group_name> -f <fold> -t <time>
```
This will instantiate a CLIP ViT-L/14 vision backbone trained using the previous step, and use its predictions as input to the LRM. The `src/run_reasoning.py` file details the process of generating the clinical narratives, and the Qwen3 model is implemented in `src/models/reasoning.py`.
